{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3addb969",
   "metadata": {},
   "source": [
    "the less threatening code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae9c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ISHAAN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Number of unique recipes: 1015\n",
      "üîß Loading model (this might take 20‚Äì30 seconds)...\n",
      "\n",
      "üîç Query -> 'Rajma Masala'\n",
      "\n",
      "üç≤ Recipe: eggplant brinjal rice vangi bhat  (Score: 0.536)\n",
      "   Ingredients: rice parboiled milled dal dal brinjal tamarind pulp asafoetida pepper black oil water distilled...\n",
      "\n",
      "üç≤ Recipe: plain khitchdi plain khichri khichdi  (Score: 0.533)\n",
      "   Ingredients: rice parboiled milled dal ginger fresh butter...\n",
      "\n",
      "üç≤ Recipe: garlic chutney poondu chutney lahasun ki chutney  (Score: 0.532)\n",
      "   Ingredients: dal tamarind pulp garlic asafoetida chillies red shallots...\n",
      "\n",
      "üç≤ Recipe: mixed pulse and vegetable salad  (Score: 0.523)\n",
      "   Ingredients: rajmah red lettuce cucumber green elongate potato big chillies green onion big pepper black beans kabuli whole dried raw...\n",
      "\n",
      "üç≤ Recipe: kidney bean sandwich with cottage cheese  (Score: 0.520)\n",
      "   Ingredients: rajmah red chillies green garlic white water unsalted...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Text Cleaning\n",
    "# -----------------------------\n",
    "def normalize_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def simplify_ingredients(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "\n",
    "    # Kill Latin names and parenthesis content\n",
    "    s = re.sub(r\"\\([^)]*\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\b[a-z]{3,10}\\s[a-z]{3,10}\\b\", \"\", s)\n",
    "    s = re.sub(r\"\\b[a-z]{5,}\\s*\\(.*?\\)\", \"\", s)\n",
    "\n",
    "    # Keyword simplifications\n",
    "    replacements = {\n",
    "        \"bengal gram\": \"chickpeas\",\n",
    "        \"black gram\": \"urad dal\",\n",
    "        \"green gram\": \"moong dal\",\n",
    "        \"kidney bean\": \"rajma\",\n",
    "        \"curd\": \"yogurt\",\n",
    "        \"garam masala powder\": \"garam masala\",\n",
    "        \"red chilli powder\": \"chili powder\",\n",
    "        \"turmeric powder\": \"turmeric\",\n",
    "        \"mustard seeds\": \"rai\",\n",
    "        \"coriander powder\": \"dhania powder\",\n",
    "        \"cumin powder\": \"jeera powder\",\n",
    "        \"cottage cheese\": \"paneer\",\n",
    "    }\n",
    "    for k, v in replacements.items():\n",
    "        s = s.replace(k, v)\n",
    "\n",
    "    # Strip weird extra tokens\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load Dataset\n",
    "# -----------------------------\n",
    "df = pd.read_excel(\"data/recipes.xlsx\")\n",
    "\n",
    "def find_col(possible_names):\n",
    "    for name in possible_names:\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "name_col = find_col([\"_recipe_name_orig\", \"recipe_name\", \"Recipe_Name\"])\n",
    "ing_col = find_col([\"ingredient_name_orig\", \"ingredient_name\", \"Ingredient_Name\"])\n",
    "food_col = find_col([\"food_name_org\", \"food_name\", \"Food_Name\"])\n",
    "\n",
    "if not name_col:\n",
    "    raise KeyError(\"No recipe name column found in your file.\")\n",
    "\n",
    "def combine_text(row):\n",
    "    ing = str(row[ing_col]) if ing_col and ing_col in row else \"\"\n",
    "    food = str(row[food_col]) if food_col and food_col in row else \"\"\n",
    "    return (ing + \" \" + food).strip()\n",
    "\n",
    "df[\"combined_text\"] = df.apply(combine_text, axis=1)\n",
    "\n",
    "grouped = (\n",
    "    df.groupby(name_col)[\"combined_text\"]\n",
    "    .apply(lambda x: \" \".join(x))\n",
    "    .reset_index(name=\"full_text\")\n",
    ")\n",
    "\n",
    "grouped[\"clean_text\"] = grouped[\"full_text\"].apply(simplify_ingredients)\n",
    "grouped[\"recipe_clean\"] = grouped[name_col].apply(normalize_text)\n",
    "\n",
    "print(\"‚úÖ Number of unique recipes:\", len(grouped))\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Embeddings\n",
    "# -----------------------------\n",
    "print(\"üîß Loading model (this might take 20‚Äì30 seconds)...\")\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "recipe_embeddings = model.encode(grouped[\"clean_text\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Search Function\n",
    "# -----------------------------\n",
    "def find_similar_recipes(query, top_k=5, threshold=0.35):\n",
    "    query_clean = normalize_text(query)\n",
    "    query_emb = model.encode(query_clean, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(query_emb, recipe_embeddings)[0]\n",
    "    top_results = scores.argsort(descending=True)\n",
    "\n",
    "    print(f\"\\nüîç Query -> '{query}'\\n\")\n",
    "    found = 0\n",
    "    for idx in top_results[:top_k * 3]:  # allow a wider net, then filter\n",
    "        idx = int(idx)\n",
    "        score = float(scores[idx])\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        found += 1\n",
    "        name = grouped.iloc[idx][\"recipe_clean\"]\n",
    "        ingredients = grouped.iloc[idx][\"clean_text\"][:180] + \"...\"\n",
    "        print(f\"üç≤ Recipe: {name}  (Score: {score:.3f})\")\n",
    "        print(f\"   Ingredients: {ingredients}\\n\")\n",
    "        if found >= top_k:\n",
    "            break\n",
    "\n",
    "    if found == 0:\n",
    "        print(\"‚ùå No strong matches found.\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Example Run\n",
    "# -----------------------------\n",
    "find_similar_recipes(\"Rajma Masala\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb23f8",
   "metadata": {},
   "source": [
    "THE NEW AND IMPROVED CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab84313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10271 rows. Using recipe-name column: 'recipe_name'. ingredient/food columns: 'ingredient_name_org', 'food_name_org'\n",
      "Number of unique recipes after grouping: 1015\n",
      "Using device: cpu. Model: all-MiniLM-L6-v2 (batch_size=64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and mapping to models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Improved, fixed, and commented semantic + fuzzy recipe search\n",
    "# Save this as search_recipes.py and run. Adjust DATA_PATH and model_name as needed.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# embeddings & fuzzy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# optional torch only used for device detection and saving/loading tensors\n",
    "import torch\n",
    "\n",
    "# ---------- config ----------\n",
    "DATA_PATH = Path(\"data/recipes.xlsx\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose model: strong but slow -> \"all-mpnet-base-v2\"; fast & decent -> \"all-MiniLM-L6-v2\"\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"   # default: faster and smaller for development\n",
    "BATCH_SIZE = 64                   # pass to model.encode to limit memory/throughput spikes\n",
    "TOP_PREFILTER = 200               # only fuzzy-match top semantic candidates\n",
    "MIN_SCORE = 0.30                  # default min combined score threshold\n",
    "SEM_FUZZY_WEIGHT = (0.7, 0.3)     # (semantic_weight, fuzzy_weight)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def normalize_text(s, keep_digits=False):\n",
    "    \"\"\"Lowercase, strip, normalize unicode, remove punctuation.\n",
    "       Set keep_digits=True if numerical tokens (e.g., '2 eggs') matter.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    if keep_digits:\n",
    "        s = re.sub(r\"[^0-9a-z\\s]\", \" \", s)\n",
    "    else:\n",
    "        s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def find_col(df, options):\n",
    "    for name in options:\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "# ---------- load data ----------\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing {DATA_PATH}. Put your recipes.xlsx in data/ or change DATA_PATH.\")\n",
    "\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# Robust column detection\n",
    "name_col = find_col(df, [\"recipe_name_orig\", \"recipe_name\", \"_recipe_name_orig\", \"RecipeName\", \"translatedRecipeName\", \"recipe_name_org\"])\n",
    "ing_col  = find_col(df, [\"ingredient_name_org\", \"ingredient_name\", \"ingredient\", \"Ingredient_Name\", \"food_name_org\", \"food_name\"])\n",
    "food_col = find_col(df, [\"food_name_org\", \"food_name\", \"food\", \"Food_Name\", \"translatedIngredients\"])\n",
    "\n",
    "if not name_col:\n",
    "    raise KeyError(\"No recipe name column found. Check your Excel headers (tried several options).\")\n",
    "\n",
    "# Combine ingredient columns (graceful to missing columns)\n",
    "def combine_row_text(row):\n",
    "    parts = []\n",
    "    if ing_col and ing_col in row and pd.notna(row[ing_col]):\n",
    "        parts.append(str(row[ing_col]))\n",
    "    if food_col and food_col in row and pd.notna(row[food_col]):\n",
    "        parts.append(str(row[food_col]))\n",
    "    return \" \".join(parts).strip()\n",
    "\n",
    "df = df.dropna(subset=[name_col]).reset_index(drop=True)\n",
    "df[\"combined_text\"] = df.apply(combine_row_text, axis=1)\n",
    "print(f\"Loaded {len(df)} rows. Using recipe-name column: '{name_col}'. ingredient/food columns: '{ing_col}', '{food_col}'\")\n",
    "\n",
    "# Group into one document per recipe (join ingredient rows)\n",
    "grouped = (\n",
    "    df.groupby(name_col)[\"combined_text\"]\n",
    "      .apply(lambda texts: \" \".join([t for t in texts.astype(str) if t.strip() != \"\"]))\n",
    "      .reset_index(name=\"full_text\")\n",
    ")\n",
    "\n",
    "# Keep original name column (string), normalized versions for encoding\n",
    "grouped[\"recipe_clean\"] = grouped[name_col].apply(lambda x: normalize_text(x if pd.notna(x) else \"\"))\n",
    "grouped[\"clean_text\"] = grouped[\"full_text\"].apply(lambda x: normalize_text(x if pd.notna(x) else \"\", keep_digits=True))\n",
    "\n",
    "print(f\"Number of unique recipes after grouping: {len(grouped)}\")\n",
    "if grouped[name_col].duplicated().any():\n",
    "    print(\"Warning: duplicate recipe names found after grouping. Consider using a unique id to distinguish.\")\n",
    "\n",
    "# ---------- load model & compute embeddings ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}. Model: {MODEL_NAME} (batch_size={BATCH_SIZE})\")\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "# Compose text for embedding: name + ingredients (you can experiment with order/sep)\n",
    "texts_to_encode = (grouped[\"recipe_clean\"] + \" \" + grouped[\"clean_text\"]).tolist()\n",
    "\n",
    "# encode with batching to avoid OOM\n",
    "recipe_embeddings = model.encode(\n",
    "    texts_to_encode,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=True,\n",
    "    device=device\n",
    ")\n",
    "# ensure float32\n",
    "if recipe_embeddings.dtype != torch.float32 and hasattr(recipe_embeddings, \"to\"):\n",
    "    recipe_embeddings = recipe_embeddings.to(torch.float32)\n",
    "\n",
    "# Save artifacts for reuse\n",
    "# - embeddings as numpy for portability\n",
    "np.save(MODELS_DIR / \"recipe_embeddings.npy\", recipe_embeddings.cpu().numpy())\n",
    "grouped.to_pickle(MODELS_DIR / \"recipes_mapping.pkl\")\n",
    "\n",
    "print(\"Saved embeddings and mapping to models/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef4bff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results:\n",
      "Curd rice (Dahi bhaat/Dahi chawal/ Perugu annam/Daddojanam/Thayir saadam)  (score=0.7277)\n",
      "  Ingredients preview: rice rice parboiled milled oryza sativa green chilli chillies green all varieties capsicum annum curry leaves curry leaves murraya koenigii ginger chopped ginger fresh zingiber officinale asafoetida asafoetida ferula assa foetida dry whole red chilli...\n",
      "\n",
      "Murmura (Puffed rice)  (score=0.6406)\n",
      "  Ingredients preview: puffed rice rice puffed oryza sativa milk milk whole cow sugar sugar white\n",
      "\n",
      "Tamarind rice (Chintapandu pulihora/Puliyodharai/Puli sadam/Huli anna)  (score=0.6236)\n",
      "  Ingredients preview: rice rice parboiled milled oryza sativa channa dal bengal gram dal cicer arietinum black gram dal black gram dal phaseolus mungo tamarind tamarind pulp tamarindus indica curry leaves curry leaves murraya koenigii asafoetida asafoetida ferula assa foe...\n",
      "\n",
      "Split bengal gram sweet rice (Channa dal sweet rice)  (score=0.6215)\n",
      "  Ingredients preview: rice rice parboiled milled oryza sativa channa dal bengal gram dal cicer arietinum jaggery jaggery cane saccharum officinarum water water distilled\n",
      "\n",
      "Mexican rice  (score=0.6037)\n",
      "  Ingredients preview: rice rice parboiled milled oryza sativa tomato tomato ripe hybrid solanum lycopersicum onion onion big allium cepa onion onion big allium cepa black cardamom cardamom black elettaria cardamomum cumin seeds cumin seeds cuminum cyminum red chilli powde...\n",
      "\n",
      "Lemon rice (Pulihora, Elumichai sadam, Chitranna)  (score=0.5976)\n",
      "  Ingredients preview: rice rice parboiled milled oryza sativa whole red chillies chillies red capsicum annum haldi turmeric powder curcuma domestica peanuts ground nut arachis hypogea mustard seeds mustard seeds brassica nigra lemon lemons flesh only raw weighed with peel...\n",
      "\n",
      "Sweet rice (Meethe chawal)  (score=0.5969)\n",
      "  Ingredients preview: rice rice parboiled milled oryza sativa raisins raisins dried black vitis vinifera green cardamom cardamom green elettaria cardamomum cloves cloves syzygium aromaticum almonds almond prunus amygdalus pistachio nuts pistachio nuts pistacla vera milk m...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- semantic + fuzzy search function ----------\n",
    "def search_recipe_semantic(query, top_k=5, min_score=MIN_SCORE, boost_keywords=None, top_prefilter=TOP_PREFILTER, sem_fuzzy_weight=SEM_FUZZY_WEIGHT):\n",
    "    \"\"\"Return top_k recipes matching `query`.\n",
    "       Strategy:\n",
    "         1) normalize and optionally boost query tokens\n",
    "         2) encode query and compute cosine similarity vs all embeddings (fast for small N)\n",
    "         3) fuzzy-match only the top_prefilter semantic candidates to blend scores\n",
    "         4) return ranked results\n",
    "    \"\"\"\n",
    "    q = normalize_text(query, keep_digits=True)\n",
    "    if boost_keywords:\n",
    "        q = q + \" \" + \" \".join(boost_keywords)\n",
    "\n",
    "    q_emb = model.encode(q, convert_to_tensor=True, device=device)\n",
    "    # compute semantic scores (cosine similarity)\n",
    "    scores = util.cos_sim(q_emb, recipe_embeddings)[0]  # tensor\n",
    "    semantic_scores = scores.cpu().numpy().flatten()\n",
    "\n",
    "    N = len(semantic_scores)\n",
    "    if top_prefilter is None or top_prefilter >= N:\n",
    "        top_idx = np.arange(N)\n",
    "    else:\n",
    "        top_idx = np.argsort(-semantic_scores)[:min(top_prefilter, N)]\n",
    "\n",
    "    combined_scores = semantic_scores.copy()  # start with pure semantic\n",
    "\n",
    "    sem_w, fuzzy_w = sem_fuzzy_weight\n",
    "    # apply fuzzy only to top candidates (saves compute)\n",
    "    for i in top_idx:\n",
    "        recipe_name = str(grouped.loc[i, name_col])\n",
    "        fuzz_score = fuzz.partial_ratio(query, recipe_name) / 100.0\n",
    "        combined_scores[i] = sem_w * semantic_scores[i] + fuzzy_w * fuzz_score\n",
    "\n",
    "    # final ranking\n",
    "    sorted_idx = np.argsort(-combined_scores)\n",
    "    out = []\n",
    "    for i in sorted_idx:\n",
    "        if len(out) >= top_k:\n",
    "            break\n",
    "        score = float(combined_scores[i])\n",
    "        if score < min_score:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"recipe_name\": grouped.loc[i, name_col],\n",
    "            \"recipe_clean\": grouped.loc[i, \"recipe_clean\"],\n",
    "            \"score\": round(score, 4),\n",
    "            \"ingredients_preview\": (grouped.loc[i, \"clean_text\"] or \"\")[:250] + (\"...\" if len(grouped.loc[i, \"clean_text\"] or \"\") > 250 else \"\")\n",
    "        })\n",
    "\n",
    "    # fallback: if nothing above threshold, return top semantic candidates (no threshold)\n",
    "    if not out:\n",
    "        fallback_idx = np.argsort(-semantic_scores)[:top_k]\n",
    "        for i in fallback_idx:\n",
    "            out.append({\n",
    "                \"recipe_name\": grouped.loc[int(i), name_col],\n",
    "                \"recipe_clean\": grouped.loc[int(i), \"recipe_clean\"],\n",
    "                \"score\": round(float(semantic_scores[int(i)]), 4),\n",
    "                \"ingredients_preview\": (grouped.loc[int(i), \"clean_text\"] or \"\")[:250] + (\"...\" if len(grouped.loc[int(i), \"clean_text\"] or \"\") > 250 else \"\")\n",
    "            })\n",
    "    return out\n",
    "\n",
    "# ---------- example ----------\n",
    "if __name__ == \"__main__\":\n",
    "    res = search_recipe_semantic(\"rice\", top_k=7, min_score=0.25, boost_keywords=[\"rice\", \"curry\", \"indian\"])\n",
    "    print(\"\\nSearch results:\")\n",
    "    for r in res:\n",
    "        print(f\"{r['recipe_name']}  (score={r['score']})\")\n",
    "        print(\"  Ingredients preview:\", r['ingredients_preview'])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e91d6",
   "metadata": {},
   "source": [
    "\n",
    "evolution instead of patching things together.\n",
    "\n",
    "\n",
    "üîπ 1. Start with the problem in the old version\n",
    "\n",
    "‚ÄúThe earlier version was functional but limited ‚Äî it relied on basic text matching and didn‚Äôt capture deeper semantic relationships between recipe descriptions and ingredient text.‚Äù\n",
    "\n",
    "This shows you‚Äôre aware of your own weaknesses ‚Äî that‚Äôs what supervisors respect.\n",
    "\n",
    "üîπ 2. Explain the motivation for the change\n",
    "\n",
    "‚ÄúTo improve recommendation accuracy and make the system more context-aware, I transitioned to a transformer-based embedding model (SentenceTransformer). This allowed me to encode semantic meaning rather than just surface-level word overlap.‚Äù\n",
    "\n",
    "You‚Äôre showing technical initiative ‚Äî that‚Äôs gold.\n",
    "\n",
    "üîπ 3. Justify the specific choice\n",
    "\n",
    "‚ÄúI initially used a simpler model, but moved to all-mpnet-base-v2 after evaluating trade-offs between speed and quality. It provides higher-quality embeddings for nuanced recipe text, which directly benefits the recommendation precision.‚Äù\n",
    "\n",
    "Shows you thought about performance vs. accuracy, not just copied something new.\n",
    "\n",
    "üîπ 4. Mention implementation refinements\n",
    "\n",
    "‚ÄúI also optimized how embeddings are generated ‚Äî combining both recipe descriptions and ingredient text, batching them efficiently, and caching embeddings to prevent redundant computations.‚Äù\n",
    "\n",
    "This signals engineering maturity.\n",
    "\n",
    "üîπ 5. Close with a measurable or observable improvement\n",
    "\n",
    "‚ÄúAs a result, recommendations are now more relevant, especially for recipes with synonyms or contextually related ingredients that the older model struggled to connect.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74522f",
   "metadata": {},
   "source": [
    "Change 1: Moving from TF-IDF (surface lexical similarity) ‚Üí dense embeddings via Transformer (semantic similarity)\n",
    "\n",
    "Sentence‚ÄëBERT: Sentence Embeddings using Siamese BERT‚ÄëNetworks by Reimers & Gurevych (2019). This is essentially the foundation for using sentence-transformer embeddings for semantic similarity. \n",
    "arXiv\n",
    "\n",
    "The performance of BERT as data representation of text clustering (2022) shows that TF-IDF fails to capture context and word order, whereas BERT-based embeddings do better for grouping/cluster tasks. \n",
    "SpringerOpen\n",
    "\n",
    "Performance of 4 Pre‚ÄëTrained Sentence Transformer Models in the Semantic Query of a Systematic Review Dataset on Peri‚ÄëImplantitis (2024) compares several sentence-transformer models showing trade-offs between speed vs accuracy (which supports your choice of MiniLM vs MPNet). \n",
    "MDPI\n",
    "\n",
    "Change 2: Combining semantic (dense embeddings) + fuzzy string matching (surface lexical) in search ranking\n",
    "\n",
    "Hybridizing Fuzzy String Matching and Machine Learning for Improved Ontology Alignment (2023) demonstrates how hybrid fuzzy + semantic approaches give improved alignment/ similarity over just lexical. \n",
    "MDPI\n",
    "\n",
    "An Improved Fusion‚ÄëBased Semantic Similarity Measure for Effective Collaborative Filtering Recommendations (2024) shows fusion of multiple similarity signals (semantic + lexical) improves recommendation accuracy. \n",
    "SpringerLink\n",
    "\n",
    "Perbandingan Metode Collaborative Filtering dan Hybrid Semantic Similarity (2025) an Indonesian paper comparing pure CF vs hybrid semantic similarity shows the benefit of combining signals (can analogously support your hybrid semantic+fuzzy). \n",
    "Jurnal Universitas Gadjah Mada\n",
    "\n",
    "Change 3: Grouping multiple rows into a single ‚Äúdocument‚Äù per recipe + embedding that combined text\n",
    "\n",
    "Text Similarity in Vector Space Models: A Comparative Study (2018) shows that longer aggregated documents tend to benefit more from semantic embeddings than simple vector-space lexical models. \n",
    "Emergent Mind\n",
    "\n",
    "Technological troubleshooting based on sentence embedding with deep transformers (2021) demonstrates embedding of longer structured document texts (e.g., manufacturing issue logs) benefits from combining multiple sentences/rows. \n",
    "SpringerLink\n",
    "\n",
    "Semantic Search with Sentence‚ÄëBERT for Design Information Retrieval (2022) uses entire document encoding rather than individual fragments, showing improved retrieval when you embed the full context. \n",
    "ntrs.nasa.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb2733",
   "metadata": {},
   "source": [
    "Here‚Äôs how you explain why you replaced TF-IDF with SentenceTransformer ‚Äî with both technical and research-based reasoning.\n",
    "\n",
    "üîπ Core Reason\n",
    "\n",
    "TF-IDF is lexical. SentenceTransformers are semantic.\n",
    "\n",
    "TF-IDF only looks at word frequency ‚Äî it can‚Äôt tell that ‚Äúpasta‚Äù and ‚Äúspaghetti‚Äù mean the same thing, or that ‚Äúgrilled chicken with herbs‚Äù is similar to ‚Äúroasted chicken seasoned with thyme.‚Äù\n",
    "SentenceTransformer models capture meaning, not just matching words.\n",
    "\n",
    "üîπ Detailed Technical Reasons\n",
    "\n",
    "Context Awareness\n",
    "\n",
    "TF-IDF treats every word independently.\n",
    "\n",
    "SentenceTransformer (based on BERT/MPNet) uses attention mechanisms that understand word meaning in context (e.g., ‚Äúapple pie‚Äù ‚â† ‚ÄúApple laptop‚Äù).\n",
    "\n",
    "So your recipe system now groups semantically similar dishes even if the text differs.\n",
    "\n",
    "Synonym & Paraphrase Handling\n",
    "\n",
    "TF-IDF breaks completely when similar items use different phrasing.\n",
    "\n",
    "SentenceTransformer embeds both into a similar vector space.\n",
    "\n",
    "Example: ‚Äúspicy curry‚Äù and ‚Äúhot masala‚Äù ‚Äî zero lexical overlap, but semantically identical.\n",
    "\n",
    "Dimensional Efficiency\n",
    "\n",
    "TF-IDF vectors are sparse and huge (tens of thousands of features).\n",
    "\n",
    "SentenceTransformer gives dense, fixed-size embeddings (typically 768D), which makes similarity calculations much faster and memory-efficient at scale.\n",
    "\n",
    "Better for Downstream Tasks (like recommendations)\n",
    "\n",
    "TF-IDF doesn‚Äôt generalize. New or unseen recipes get poor matches because vocabulary overlap drives everything.\n",
    "\n",
    "SentenceTransformer generalizes meaning ‚Äî a completely new recipe can still find close semantic neighbors even if words differ.\n",
    "\n",
    "Empirical Backing\n",
    "\n",
    "According to Reimers & Gurevych (2019), sentence-BERT embeddings outperform TF-IDF and averaged word vectors by a wide margin in semantic similarity tasks.\n",
    "\n",
    "Multiple 2022‚Äì2024 studies confirm that transformer embeddings yield higher cosine similarity correlation with human judgments.\n",
    "\n",
    "üîπ How to Phrase It to the Supervisor\n",
    "\n",
    "‚ÄúI replaced TF-IDF with SentenceTransformer because TF-IDF only captures surface-level word frequency. The new model captures contextual and semantic similarity, letting the recommender recognize related dishes even when the exact words differ. This shift improves both accuracy and generalization ‚Äî it‚Äôs backed by multiple studies showing transformers outperform TF-IDF on semantic similarity tasks.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0e9b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
